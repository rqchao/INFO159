{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rqchao/INFO159/blob/main/HW3/HW_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejZ2oE4GmdfF"
      },
      "source": [
        "# Homework 3: Language Models, Contextual Embedding and BERT\n",
        "\n",
        "In this homework, we will explore implementations of various language models we saw in lecture. We will explore BERT and measure perplexity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj0EWMnOmgWw"
      },
      "source": [
        "##Set Up\n",
        "\n",
        "If you're opening this Notebook on colab, you will probably need to install Transformers. Make sure your version of Transformers is at least 4.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svbBi3YGmMJX",
        "outputId": "623f3b76-d7cf-4d60-e001-cb54ba9d203d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (4.25.1)\n",
            "Requirement already satisfied: requests in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhnQ3PoVnH7D",
        "outputId": "3781372d-8601-4755-81ca-bef06854b6ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\chaoq\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.24.0\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEDSQsLnbLm"
      },
      "source": [
        "IMPORTANT: For this assignment, GPU is not necessary. The following code block should show \"Running on cpu\". \n",
        "Go to Runtime > Change runtime type > Hardware accelerator > None if otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7osYx6Hm0vh",
        "outputId": "aa8ad737-22e4-4e54-dad8-8f00241135d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ4XlcOvIHgl"
      },
      "source": [
        "# Masking\n",
        "\n",
        "One of the core ideas to wrap your head around with transformer-based language models (and PyTorch) is the concept of *masking*---preventing a model from seeing specific tokens in the input during training.\n",
        "\n",
        "* BERT training relies on the concept of *masked language modeling*: masking a random set of input tokens in a sequence and attempting to predict them.  Remember that BERT is *bidirectional*, so that it can use all of the other non-masked tokens in a sentence to make that prediction.\n",
        "\n",
        "* The GPT class of models acts as a traditional left-to-right language model (sometimes called a \"causal\" LM) .  This family also uses self-attention based transformers---but, when making a prediction for the word $w_i$ at position $i$, it can only use information about words $w_1, \\ldots, w_{i-1}$ to do so.  All of the other tokens following position $i-1$ must be *masked* (hidden from view).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi0LW-vKZnIH"
      },
      "source": [
        "Think about a mask as a matrix that's applied to every input $w$ when generating an output $o$ that determines whether an given $o_i$ is allowed to access each token in $w$.  For example, when passing a three-word input sequence through a transformer (to yield a three-word output sequence), a mask is a $3 \\times 3$ matrix where the cells are essentially  answering the following questions:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "o_1 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_1 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_1 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "o_2 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_2 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_2 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "o_3 \\; \\textrm{hide} \\; w_1\\textrm{?} & o_3 \\; \\textrm{hide} \\; w_2\\textrm{?} & o_3 \\; \\textrm{hide} \\; w_3\\textrm{?} \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "In the masks we will consider below, 1 denotes that a position should be hidden; 0 denotes that it should be visible. Consider this mask:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 1 \\\\\n",
        "1 & 0 & 1 \\\\\n",
        "1 & 1 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "And consider this sequence:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "\\textrm{John} & \\textrm{likes}  & \\textrm{dogs}  \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "When applying this mask to that sequence, we're saying that when we're generating the output for $o_1$ (*John*), we can only consider $w_1$ as an input (*John*).  Likewise, when we generate the output for $o_2$ (*likes*), we can only consider $w_2$ as an input (*likes*), and so on.  (This is a terrible mask!  But illustrates what function a mask performs.)\n",
        "\n",
        "The following code illustrates how this works for that particular mask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "Ea2E6zlMZkzO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def visualize_masking(sequences, mask):\n",
        "  \n",
        "  print(mask)\n",
        "  for sequence in sequences:\n",
        "    for i in range(len(sequence)):\n",
        "      visible=[]\n",
        "      for j in range(len(sequence)):\n",
        "        if mask[i][j]==0:\n",
        "          visible.append(sequence[j])\n",
        "      print(\"for word %s, the following tokens are visible: %s\" % (sequence[i], visible))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xROu5KmvKMua",
        "outputId": "285a3617-dd95-4e7e-cb33-0d7c882d12e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]]\n",
            "for word This, the following tokens are visible: ['This']\n",
            "for word is, the following tokens are visible: ['is']\n",
            "for word a, the following tokens are visible: ['a']\n",
            "for word sentence, the following tokens are visible: ['sentence']\n",
            "for word that, the following tokens are visible: ['that']\n",
            "for word has, the following tokens are visible: ['has']\n",
            "for word exactly, the following tokens are visible: ['exactly']\n",
            "for word ten, the following tokens are visible: ['ten']\n",
            "for word tokens, the following tokens are visible: ['tokens']\n",
            "for word ., the following tokens are visible: ['.']\n",
            "\n",
            "for word Here's, the following tokens are visible: [\"Here's\"]\n",
            "for word another, the following tokens are visible: ['another']\n",
            "for word sequence, the following tokens are visible: ['sequence']\n",
            "for word with, the following tokens are visible: ['with']\n",
            "for word 10, the following tokens are visible: ['10']\n",
            "for word words, the following tokens are visible: ['words']\n",
            "for word like, the following tokens are visible: ['like']\n",
            "for word the, the following tokens are visible: ['the']\n",
            "for word last, the following tokens are visible: ['last']\n",
            "for word ., the following tokens are visible: ['.']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sequences=[[\"This\", \"is\", \"a\", \"sentence\", \"that\", \"has\", \"exactly\", \"ten\", \"tokens\", \".\"], [\"Here's\", \"another\", \"sequence\", \"with\", \"10\", \"words\", \"like\", \"the\", \"last\", \".\"]]\t\n",
        "\n",
        "seq_length=len(sequences[0])\n",
        "\n",
        "test_mask=np.ones((seq_length,seq_length))\n",
        "for i in range(seq_length):\n",
        "  test_mask[i,i]=0\n",
        "\n",
        "visualize_masking(sequences, test_mask)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKv3h625eMo3"
      },
      "source": [
        "##Q1.  \n",
        "As we discussed in class, BERT masks a random set of words in the input and attempts to reconstruct those words as output.  Create a mask that randomly masks token positions 2 and 7 (for an input sequence length of 10 tokens, with 0 being the position of the first token).  For an input sequence of 10 tokens, you should generate output representations for all 10 tokens (i.e., $[o_1, \\ldots, o_{10}]$ in the notation above, but each representation must ignore the same 2 input tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "_31mkhIke-YI"
      },
      "outputs": [],
      "source": [
        "def create_bert_mask(seq_length):\n",
        "  mask=np.zeros((seq_length,seq_length))\n",
        "  # implement BERT mask here\n",
        "  \n",
        "  # BEGIN SOLUTION\n",
        "  for i in range(seq_length):\n",
        "    mask[i,1]=1\n",
        "    mask[i,6]=1\n",
        "  # END SOLUTION\n",
        "\n",
        "  return mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKjasreUfLww"
      },
      "source": [
        "##Q2\n",
        "A left-to-right language model (such as GPT) can only use information from input words $[w_1, \\ldots, w_{i}]$ when generating the representation for output $o_i$.  Encode this as a mask as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "svKuQyQff4fK"
      },
      "outputs": [],
      "source": [
        "def create_causal_mask(seq_length):\n",
        "  mask=np.zeros((seq_length,seq_length))\n",
        "  # implement causal mask here\n",
        "\n",
        "  # BEGIN SOLUTION\n",
        "  for i in range(seq_length):\n",
        "    for j in range(i+1,seq_length):\n",
        "      mask[i,j]=1\n",
        "  # END SOLUTION\n",
        "  sequences=[[\"This\", \"is\", \"a\", \"sentence\", \"that\", \"has\", \"exactly\", \"ten\", \"tokens\", \".\"], [\"Here's\", \"another\", \"sequence\", \"with\", \"10\", \"words\", \"like\", \"the\", \"last\", \".\"]]\t\n",
        "  visualize_masking(sequences, mask)\n",
        "  \n",
        "  return mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXRkI2DZnwvB"
      },
      "source": [
        "Now let's go ahead and embed these masks within a model.  First, we'll load some textual data (from Austen's *Pride and Prejudice*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "cgTwRcqDn6ni"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/files/1342/1342-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "7v2EJeEBobD8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "u8hjeySdojyr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\chaoq\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 204,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jre3sSWgvnwi"
      },
      "source": [
        "Let's read in the data and tokenize it; for this homework, we'll only work with the first 10,000 tokens of that book; we'll keep only the most frequent 1,000 word types (all other tokens will be mapped to an [UNK] token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "7kzw9IcFoDZq"
      },
      "outputs": [],
      "source": [
        "def read_data(filename):\n",
        "  with open(filename) as file:\n",
        "    data=file.read().lower()\n",
        "    first10K=' '.join(data.split(\" \")[:10000])\n",
        "    toks=nltk.word_tokenize(first10K)[:10000]\n",
        "    vocab={\"[PAD]\":0, \"[UNK]\":1}\n",
        "    counts=Counter()\n",
        "    for tok in toks:\n",
        "      counts[tok]+=1\n",
        "    for v, _ in counts.most_common(1000):\n",
        "      vocab[v]=len(vocab)\n",
        "    tokids=[]\n",
        "    for tok in toks:\n",
        "      tokid=1\n",
        "      if tok in vocab:\n",
        "        tokid=vocab[tok]\n",
        "      \n",
        "      tokids.append(tokid)\n",
        "\n",
        "    return tokids, vocab  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r_XHBZPv4kD"
      },
      "source": [
        "Now let's specify our model in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "p74L0s1DsKa2"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class MaskedLM(nn.Module):\n",
        "    def __init__(self, vocab, mask, d_model=512):       \n",
        "        super().__init__()\n",
        "        self.vocab=vocab\n",
        "        self.mask=mask\n",
        "        vocab_size=len(vocab)\n",
        "        self.embeddings=nn.Embedding(1002,512)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead=8, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        self.linear=torch.nn.Linear(d_model, vocab_size)\n",
        "        self.rev_vocab={vocab[k]:k for k in vocab}\n",
        "\n",
        "    def forward(self, input): \n",
        "        # first we pass the input word IDS through an embedding layer to get embeddings for them\n",
        "        input=self.embeddings(input)\n",
        "        # then we pass those embeddings through a transformer to get contextual representations, masking the input where appropriate\n",
        "        out = self.transformer_encoder.forward(input, mask=self.mask)        \n",
        "        # finally we pass those embeddings through a linear layer to transform it into the output space (the size of our vocabulary)\n",
        "        h=self.linear(out)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "njdPnGVQsQOl"
      },
      "outputs": [],
      "source": [
        "def get_batches(xs, ys, batch_size=32):\n",
        "    batch_x=[]\n",
        "    batch_y=[]\n",
        "    for i in range(0, len(xs), batch_size):\n",
        "        batch_x.append(torch.LongTensor(xs[i:i+batch_size]).to(device))\n",
        "        batch_y.append(torch.LongTensor(ys[i:i+batch_size]).to(device))\n",
        "    return batch_x, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "xm6tLzHmtAfN"
      },
      "outputs": [],
      "source": [
        "tokids, vocab=read_data(\"1342-0.txt\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "9822mHI3sR1Q"
      },
      "outputs": [],
      "source": [
        "def train(mask, data_function, tokids, vocab):\n",
        "\n",
        "    mask=torch.BoolTensor(mask).to(device)\n",
        "\n",
        "    num_labels=len(vocab)\n",
        "    model=MaskedLM(vocab, mask).to(device)\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "    cross_entropy=nn.CrossEntropyLoss()\n",
        "    losses=[]\n",
        "\n",
        "    xs, ys=data_function(tokids)\n",
        "\n",
        "    batch_x, batch_y=get_batches(xs, ys)\n",
        "\n",
        "    for epoch in range(1):\n",
        "        model.train()\n",
        "        \n",
        "        for x, y in list(zip(batch_x, batch_y)):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred=model.forward(x)\n",
        "            loss=cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
        "            losses.append(loss.item())\n",
        "            print(loss)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CXKNwLBwBG6"
      },
      "source": [
        "Our model and training process are now all defined; all that remains is to pass our inputs and outputs through it to train.  Your job here is to create the correct inputs (x) and outputs (y) to train a left-to-right (causal) language model.\n",
        "\n",
        "##Q3\n",
        "Write a function that takes in a sequence of token ids $[w_1, \\ldots, w_n]$ and segments it into 8-token chunks -- e.g., $x_1=[w_1, \\ldots, w_8]$, $x_2=[w_9, \\ldots, w_{16}]$, etc.  For each $x_i$, also create its corresponding $y_i$.  Given this language modeling specification, each $y_i$ should also contain 8 values (for each token in $x_i$).  Keep in mind this is a left-to-right causal language model; your job is to figure out the values of y that respects this design.  At token position $i$, when a model has access to $[w_1, \\ldots, w_i]$, which is the true $y_i$ for that position? Each element in $y$ should be a word ID (i.e., an integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "JIO8M9AeszAi"
      },
      "outputs": [],
      "source": [
        "def get_causal_xy(data, max_len=8):\n",
        "    xs=[]\n",
        "    ys=[]\n",
        "    # BEGIN SOLUTION\n",
        "    xs = [data[i:i+max_len] for i in range(0, len(data), max_len)]\n",
        "    ys = [data[i+1:i+max_len+1] for i in range(0, len(data), max_len)]\n",
        "    \n",
        "    if len(xs[-1]) < max_len:\n",
        "      xs = xs[:-1]\n",
        "      ys = ys[:-1]\n",
        "    return xs, ys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "AO4WOusZsWsC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(7.0310, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4619, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3741, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1574, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2277, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0824, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.9969, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0274, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1891, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.5385, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2831, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.6186, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.4228, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2465, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0395, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1205, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0118, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.8325, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7379, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2983, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3339, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2319, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.5574, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9206, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "seq_length=8\n",
        "\n",
        "train(create_causal_mask(seq_length=seq_length), get_causal_xy, tokids, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2P8GU1VsKLn"
      },
      "source": [
        "##Q4 (Write-up)  \n",
        "In this model, as implemented, does the following equivalence hold?\n",
        "\n",
        "$$\n",
        "P(y_4 \\mid w_1 = \\textrm{go}, w_2=\\textrm{ahead}, w_3=\\textrm{make}, w_4=\\textrm{my})= P(y_4 \\mid w_1 = \\textrm{ahead}, w_2=\\textrm{my}, w_3=\\textrm{make}, w_4=\\textrm{go})\n",
        "$$\n",
        "\n",
        "Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pZpVrqchSAI"
      },
      "source": [
        "# Perplexity\n",
        "To evaluate how good our language model is, we use a metric called perplexity. The perplexity of a language model (PP) on a test set is the inverse probability of the test set, normalized by the number of words. Let $W = w_{1}w_{2}\\dots w_{N}$. Then,\n",
        "\n",
        "$$PP(W) = \\sqrt[N]{\\prod_{i = 1}^{N}\\frac{1}{P(w_{i}|w_{1}\\dots w_{i - 1})}}$$\n",
        "\n",
        "However, since these probabilities are often small, taking the inverse and multiplying can be numerically unstable, so we often first compute these values in the log domain and then convert back. So this equation looks like:\n",
        "\n",
        "$$\\ln PP(W) = \\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})$$\n",
        "\n",
        "$$\\implies PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i}|w_{1}\\dots w_{i - 1})}$$\n",
        "\n",
        "Here we want to calculate the perplexity of [pretrained BERT model](https://huggingface.co/bert-base-uncased) on text from different sources. When calculating perplexity with BERT, we'll use a related measure of pseudo-perplexity, which allow us to condition on the bidirectional context (and not just the left context, as in standard perplexity):\n",
        "\n",
        "$$PP(W) = e^{\\frac{1}{N} \\sum_{i = 1}^{N} -\\ln P(w_{i} \\mid w_{1}\\dots w_{i - 1}, w_{i+1}, \\ldots, w_n)}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2MXSfzlsHp_"
      },
      "source": [
        "First, let's instantiate a BERT model, along with its WordPiece tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxyJaS1-cP3R",
        "outputId": "e288a86f-3618-427b-877f-730fbcff6889"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model=model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLAWTU8sTqf"
      },
      "source": [
        "Let's see how the BERT tokenizer tokenizes a sentence into a sequence of WordPiece ids.  Note how BERT tokenization automatically wraps an input sentences with [CLS] and [SEP] tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxlOsB0NshJm",
        "outputId": "1c247290-6294-46e8-c279-37fd9a25a1bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[ 101, 1037, 3899, 5565, 2006, 7733,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "tensor([[ 101, 1037, 3899, 5565, 2006, 7733,  102]])\n",
            "['[CLS]', 'a', 'dog', 'landed', 'on', 'mars', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"A dog landed on Mars\"\n",
        "tensor_input = tokenizer(sentence, return_tensors=\"pt\")\n",
        "print(tensor_input)\n",
        "tensor_input_ids = tensor_input[\"input_ids\"]\n",
        "print(tensor_input_ids)\n",
        "print(tokenizer.convert_ids_to_tokens(tensor_input_ids[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGu38_DB1vsB"
      },
      "source": [
        "Now let's see how we can calculate output probabilities using this model.  The output of each token position $i$ gives us $P(w_i \\mid w_1, \\ldots, w_n)$---the probability of the word at that position over our vocabulary, given *all* of the words in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvuNw52m15OU",
        "outputId": "a8ef51e6-75a6-41db-80e8-06796973ed19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 7, 30522])\n",
            "[CLS]\t101\t0.00000\n",
            "a\t1037\t0.99281\n",
            "dog\t3899\t0.99052\n",
            "landed\t5565\t0.99809\n",
            "on\t2006\t0.99874\n",
            "mars\t7733\t0.00133\n",
            "[SEP]\t102\t0.00000\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  output = model(tensor_input_ids)\n",
        "  logits = output.logits\n",
        "  # logits here are the unnormalized scores, so let's pass them through the softmax \n",
        "  # to get a probability distribution\n",
        "  softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "  # for one input sequence, the shape of the resulting distribution is: \n",
        "  # 1 x [length of input, in WordPiece tokens] x (the size of the BERT vocabulary)\n",
        "  print(softmax.shape) # [1, 7, 30522]\n",
        "  input_ints=tensor_input_ids.numpy()[0]\n",
        "  # Let's print the probability of the true inputs\n",
        "  wp_tokens=tokenizer.convert_ids_to_tokens(input_ints)\n",
        "  for i in range(len(input_ints)):\n",
        "    prob=softmax[0][i][input_ints[i]].numpy()\n",
        "    print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F90uSrL_1vot"
      },
      "source": [
        "Note that $w_i$ is in the range $[w_1, \\ldots, w_n]$ -- clearly the probability of a word is going to be high when we can observe it in the input! Let's do some masking to calculate $P(w_i \\mid w_1, \\ldots w_{i-1}, w_{i+1}, w_n)$.  Now annoyingly, BERT's `attention_mask` function only works for padding tokens; to mask input tokens, we need to intervene in the input and replace a WordPiece token that we're predicting with a special [MASK] token (BERT tokenizer word id `103`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77EdDCtk3MSa",
        "outputId": "6267e000-0a59-4d87-aaa5-3f2b0df16253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The second word here now is [MASK] token ID '103':  tensor([[ 101,  103, 3899, 5565, 2006, 7733,  102]])\n",
            "a\t1037\t0.13965\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "with torch.no_grad():\n",
        "  # let's make a copy of the original word ids so we can mask one of the tokens\n",
        "  masked_input_ids=copy.deepcopy(tensor_input_ids)\n",
        "  # we'll mask the second word\n",
        "  masked_input_ids[0][1]=tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "\n",
        "  print(\"The second word here now is [MASK] token ID '103': \", masked_input_ids)\n",
        "\n",
        "  # now let's run that through BERT in the same way we did before\n",
        "  output = model(masked_input_ids)\n",
        "  logits = output.logits\n",
        "\n",
        "  softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "  input_ints=tensor_input_ids.numpy()[0]\n",
        "\n",
        "  wp_tokens=tokenizer.convert_ids_to_tokens(input_ints)\n",
        "  i=1\n",
        "  prob=softmax[0][i][input_ints[i]].numpy()\n",
        "  print(\"%s\\t%s\\t%.5f\" % (wp_tokens[i], input_ints[i], prob))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSdj7onm1vln"
      },
      "source": [
        "You can see the probability of \"a\" as the second token has gone down to 0.13965 when we mask it.  This is the $P(w_1 =\\textrm{a} \\mid w_0, w_2, \\ldots, w_n)$.  At this point you should have everything you need to calculate the BERT pseudo-perplexity of an input sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA_eHLic_OfM"
      },
      "source": [
        "##Q5\n",
        "Implement the pseudo-perplexity measure described above, calculating the perplexity for a given model, tokenizer, and sentence. \n",
        "\n",
        "The function calculates the average probability of each token in the sentence given all the other tokens. We need to predict the probability of each word in a sentence by masking the one word to predict. Note that you should not include the probabilities of the [CLS] and [SEP] tokens in your perplexity equation -- those tokens are not part of the original test sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "GDZ9Qe8L-uUt"
      },
      "outputs": [],
      "source": [
        "# This function calculates the perplexity of a language model, given a sentence and its corresponding tokenizer\n",
        "\n",
        "# Inputs:\n",
        "# model: language model being used to calculate the perplexity\n",
        "# tokenizer: tokenizer that is used to preprocess the input sentence\n",
        "# sentence: input sentence string for which perplexity is to be calculated\n",
        "\n",
        "# Outputs:\n",
        "# returns perplexity of the input sentence\n",
        "\n",
        "def perplexity(model, tokenizer, sentence):\n",
        "\n",
        "  # hints: you'll need to:\n",
        "  # encode the input sentence using the tokenizer\n",
        "  # for each WordPiece token in the sentence (except [CLS] and [SEP]), mask that single token and \n",
        "  # calculate the probability of that true word at the masked position\n",
        "  # don't calculate perplexity for the [CLS] and [SEP] tokens (which are not part of the original test sentence).\n",
        "  perplexity=None\n",
        "  # BEGIN SOLUTION\n",
        "  perplexity = 0\n",
        "  tensor_input = tokenizer(sentence, return_tensors=\"pt\")\n",
        "  tensor_input_ids = tensor_input[\"input_ids\"]\n",
        "  input_len=len(tensor_input_ids.numpy()[0])\n",
        "  # Loop over each token in the sentence\n",
        "  for i in range(1, input_len - 1):\n",
        "    # Mask the current token\n",
        "    masked_input_ids=copy.deepcopy(tensor_input_ids)\n",
        "    masked_input_ids[0][i]=tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "    output = model(masked_input_ids)\n",
        "    logits = output.logits\n",
        "\n",
        "    softmax = torch.nn.functional.softmax(logits, dim = -1)\n",
        "    input_ints=tensor_input_ids.numpy()[0]\n",
        "    prob=softmax[0][i][input_ints[i]].detach().numpy()\n",
        "    log_prob = np.log(prob)\n",
        "    perplexity -= log_prob\n",
        "  # Calculate the average perplexity\n",
        "  perplexity /= (input_len - 2)\n",
        "  # Convert the average perplexity from log space to perplexity space\n",
        "  perplexity = np.exp(perplexity)\n",
        "  # END SOLUTION\n",
        "\n",
        "  return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "tN7HVo_oIWXF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0596667966797595\n"
          ]
        }
      ],
      "source": [
        "print(perplexity(sentence='London is the capital of the United Kingdom.', model=model, tokenizer=tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsHLgqH0-6_t"
      },
      "source": [
        "# No credit.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5eR8EF12v1s"
      },
      "source": [
        "We provide [texts](https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt) from 4 different sources ([Wikipedia](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots), [Yelp](https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset), [Fiction](https://github.com/dbamman/litbank), [Twitter](https://github.com/dbamman/anlp21/blob/main/data/potus_tweets.json)) collected from open-source datasets. Each category has 125 entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnkdWOu1BGjP",
        "outputId": "8d52da78-b007-4fe2-8071-f6e0b6e7cbb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-02-11 04:41:01--  https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt\n",
            "Resolving people.ischool.berkeley.edu (people.ischool.berkeley.edu)... 128.32.78.16\n",
            "Connecting to people.ischool.berkeley.edu (people.ischool.berkeley.edu)|128.32.78.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 61117 (60K) [text/plain]\n",
            "Saving to: ‘text_from_different_sources.txt’\n",
            "\n",
            "\r          text_from   0%[                    ]       0  --.-KB/s               \rtext_from_different 100%[===================>]  59.68K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-02-11 04:41:02 (2.02 MB/s) - ‘text_from_different_sources.txt’ saved [61117/61117]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://people.ischool.berkeley.edu/~dbamman/text_from_different_sources.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpCpGQpG4OJZ",
        "outputId": "65f5b28a-8700-427f-db0a-a7347c7bc29a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wikipedia 125\n",
            "Yelp 125\n",
            "Fiction 125\n",
            "Twitter 125\n"
          ]
        }
      ],
      "source": [
        "text_by_genre={}\n",
        "with open('text_from_different_sources.txt') as file:\n",
        "  file.readline()\n",
        "  for line in file:\n",
        "    cols=line.rstrip().split(\"\\t\")\n",
        "    genre=cols[0]\n",
        "    text=cols[1]\n",
        "\n",
        "    if genre not in text_by_genre:\n",
        "      text_by_genre[genre]=[]\n",
        "    text_by_genre[genre].append(text)\n",
        "\n",
        "for genre in text_by_genre:\n",
        "  print(genre, len(text_by_genre[genre]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtE7Umzy9xRj"
      },
      "source": [
        "Calculate perplexity on each genre over all of the words present within it; each line contains exactly one sentence for each genre.\n",
        "\n",
        "The output perplexity_by_genre = {} is a dictionary mapping genre to a list of perplexities for each sentence in that genre. For computational purpose, we only take the first 25 sentences as an example (still this can take up to 10 minutes to run), feel free to change 25 to smaller numbers.\n",
        "\n",
        "e.g. perplexity_by_genre['Wikipedia'] should be a list of 25 perplexities (one for each Wikipedia row in the input file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09NqLCO0523E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def calculate_perplexity_by_genre(text_by_genre):\n",
        "  perplexity_by_genre = {}\n",
        "  for genre in text_by_genre:\n",
        "    perplexity_by_genre[genre] = []\n",
        "    for text in text_by_genre[genre][:25]: # change 25 to smaller numbers if necessary\n",
        "      p = perplexity(sentence=text, model=model, tokenizer=tokenizer)\n",
        "      perplexity_by_genre[genre].append(p)\n",
        "  return perplexity_by_genre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sXsSGbd730C"
      },
      "outputs": [],
      "source": [
        "# running this might take up to 10 minutes\n",
        "perplexity_by_genre = calculate_perplexity_by_genre(text_by_genre)\n",
        "for genre in perplexity_by_genre:\n",
        "  print(\"Genre:\",genre,\", mean perplexity:\",np.mean(perplexity_by_genre[genre]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWk0jHUM8UFP"
      },
      "source": [
        "##Question: \n",
        "What do you think are the reasons for the wide variation in perplexity of different categories of corpus? (hint: think about the training data of the pre-trained BERT model)\n",
        "\n",
        "\n",
        "Which of these is a true language model, and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "objective: Determine if a list of 5-element tuples represents a sentence with 'projective' dependency tree\n",
        "\n",
        "params: tokens is a list of (idd, tok, pos, head, lab) for a sentence\n",
        "return True if and only if the sentence has a projective dependency tree\n",
        "\"\"\"\n",
        "# instructions\n",
        "# there is more than one way to carry this out correctly. \n",
        "# We recommend the following procedure:\n",
        "# Part 1\n",
        "# Iterate over the tuples and store their relevant components a dictionary\n",
        "# with the tuples' id (\"idd\") as the key and its head (\"head\") as the value\n",
        "# this dictionary should thus contain dependency relations in the form {idd: head}\n",
        "# Part 2\n",
        "# For every dependency relation in that dictionary, extract the head\n",
        "# establish the left and right bounds for your dependency-path search, with\n",
        "# the left bound being: 1 + the smaller of (head, idd) and \n",
        "# the right bound, conversely, being the bigger of (head, idd)\n",
        "# Part 3\n",
        "# Check if every word in the tree is reachable by following the path of dependencies\n",
        "# complete the reachable method\n",
        "\n",
        "# hints\n",
        "# -the left bound gets a 1 added to it because you don't need to check for crossing edges \n",
        "#   when you have a token that's directly besides its head\n",
        "# - a token with '0' value for its head denotes that it is the root of the sentence\n",
        "# -if any word in the tree is not reachable, this means there is non-projectivity\n",
        "#   which is when your method needs to return False.\n",
        "# -otherwise, return True!\n",
        "# -you are welcome to remove any and all of our starter code and create your own logic for this\n",
        "# if you do so, please *do not* remove the BEGIN / END Solution flags!\n",
        "def is_projective(tokens):\n",
        "\n",
        "    # BEGIN SOLUTION\n",
        "    def reachable(i, head, heads):\n",
        "        if i == head:\n",
        "            return True\n",
        "        elif i not in heads:\n",
        "            return False\n",
        "        else:\n",
        "            return reachable(heads[i], head, heads)\n",
        "    \n",
        "    heads = {} # populate this (Part 1)\n",
        "    for idd, tok, pos, head, lab in tokens:\n",
        "        heads[idd] = head\n",
        "\n",
        "    for dep_to_check in heads: # populate this (Part 2)\n",
        "        head = heads[dep_to_check]\n",
        "        left = min(head, dep_to_check)\n",
        "        right = max(head, dep_to_check)\n",
        "        for i in range(left + 1, right):\n",
        "          if not reachable(i, head, heads):\n",
        "            return False\n",
        "            \n",
        "    return True\n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "objective: perform the SHIFT operation\n",
        "\n",
        "params:\n",
        "    - wbuffer: input buffer\n",
        "    - stack: what we're buliding our parse on\n",
        "    - arcs: our dependency parse for a single token (containing the dependency relation, head-token ID, its own token ID)\n",
        "    - configurations: state of the parse at a given decision point\n",
        "    - gold_transitions: the operations we're applying at each decision point\n",
        "\"\"\"\n",
        "\n",
        "# instructions\n",
        "# 1. update configurations\n",
        "# 2. update gold_transitions\n",
        "# 3. remove word from front of buffer and push it onto stack\n",
        "# hints\n",
        "# -this can be completed in just a few lines.\n",
        "# -we have provided the code for updating configurations, so you just need to\n",
        "# update gold_transitions, then the stack and wbuffer accordingly\n",
        "# -since this is a SHIFT operation, arcs does not come into play because no head-dependent relationships are being asserted\n",
        "# -the .pop() operation removes an item at the given index from a list\n",
        "# defaulting to the last element in the list when no index is provided\n",
        "\n",
        "def perform_shift(wbuffer, stack, arcs, configurations, gold_transitions):    \n",
        "    \n",
        "    # BEGIN SOLUTION\n",
        "    # update configurations (Part 1)\n",
        "    configurations.append((list(wbuffer), list(stack), list(arcs)))\n",
        "\n",
        "    # update gold_transitions (Part 2)\n",
        "    gold_transitions.append('SHIFT')\n",
        "\n",
        "    # remove word from front of buffer and push it onto stack (Part 3)\n",
        "    stack.append(wbuffer.pop())\n",
        "    \n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "objective: perform LEFTARC_ and RIGHTARC_ operations\n",
        "\n",
        "params:\n",
        "    - direction: {\"LEFT\", \"RIGHT\"}\n",
        "    - dep_label: label for the dependency relations\n",
        "    - wbuffer: input buffer\n",
        "    - stack: what we're buliding our parse on\n",
        "    - arcs: our dependency parse for a single token (containing the dependency relation, head-token ID, its own token ID)\n",
        "    - configurations: state of the parse at a given decision point\n",
        "    - gold_transitions: the operations we're applying at each decision point\n",
        "\"\"\"\n",
        "\n",
        "# instructions\n",
        "# 1. update configurations\n",
        "# 2. update transitions\n",
        "# 3. use the first 2 elements on the stack to create a (head, child) pairing, based on the type of shift we're doing\n",
        "# 4. update arcs\n",
        "# 5. update the stack\n",
        "# hints\n",
        "# -updating configurations is identical to the perform_shift function\n",
        "# -be sure the information you're inserting into gold_transitions is formatted properly (see above)\n",
        "# -be sure that the orientation of the (head,child) pairing is correct\n",
        "\n",
        "def perform_arc(direction, dep_label, wbuffer, stack, arcs, configurations, gold_transitions):\n",
        "\n",
        "    # update configurations (Part 1)\n",
        "    configurations.append((list(wbuffer), list(stack), list(arcs)))\n",
        "\n",
        "    # update transitions (Part 2)\n",
        "    gold_transitions.append(f\"{direction}ARC_{dep_label}\")\n",
        "\n",
        "    # setup head and child items (Part 3)\n",
        "    if direction == \"LEFT\":\n",
        "        head = stack[-1]\n",
        "        child = stack[-2]\n",
        "    elif direction == \"RIGHT\":\n",
        "        head = stack[-2]\n",
        "        child = stack[-1]\n",
        "\n",
        "    # update arcs (Part 4)\n",
        "    arcs.append((dep_label, head, child))\n",
        "\n",
        "    # update stack (Part 5)\n",
        "    if direction == \"LEFT\":\n",
        "        stack.pop(-2)\n",
        "    elif direction == \"RIGHT\":\n",
        "        stack.pop()\n",
        "\n",
        "    return wbuffer, stack, arcs, configurations, gold_transitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "objective: based on inputs, return the correct configurations and actions for the parser.\n",
        "\n",
        "params:\n",
        "wbuffer: a list of word indices; the top of buffer is at the end of the list\n",
        "stack: a list of word indices; the top of buffer is at the end of the list\n",
        "arcs: a list of (label, head, dependent) tuples\n",
        "deps: the existing dependency tree\n",
        "\n",
        "Return configurations and gold_transitions\n",
        "\"\"\"\n",
        "\n",
        "# instructions\n",
        "# Initialize return lists\n",
        "# Check for simple cases (that we've translated the tree entirely, or that we need to get more words onto the stack)\n",
        "# Check for left-arc, if yes carry out\n",
        "# Check for right-arc, elif yes, carry out: your job is to fill in this portion of the code!\n",
        "# else, perform a shift\n",
        "# hints\n",
        "# -checking against conditions for the right arc operation is fairly similar to the left arc check\n",
        "#  with the order of stack1/stack2 swapped\n",
        "# -to check if all the dependents of s1 have been assigned, you can\n",
        "#  look for the presence of its children in dep_arcs\n",
        "# -don't forget to update dep_arc after you called your perform_arc function\n",
        "\n",
        "def tree_to_actions(wbuffer, stack, arcs, deps):\n",
        "\n",
        "    # Initialize return lists\n",
        "    \n",
        "    # A list of 3-element tuples of lists\n",
        "    # [(wbuffer1, stack1, arcs1), (wbuffer2, stack2, arcs2), ...]\n",
        "    # Keeps track of the states at each step\n",
        "    configurations=[]\n",
        "\n",
        "    # gold_transitions:\n",
        "    # A list of action strings, e.g [\"SHIFT\", \"LEFTARC_nsubj\"]\n",
        "    # Keeps track of the actions at each step\n",
        "    gold_transitions=[]\n",
        "\n",
        "    # Keeps track of the dependents of each word in the tree\n",
        "    # that have already been assigned\n",
        "    dep_arcs = {}\n",
        "    \n",
        "    while len(wbuffer) >= 0:\n",
        "        # Check for base-cases\n",
        "\n",
        "        # firstly, check if we have translated all the tree to transition instructions\n",
        "        if len(wbuffer) == 0 and len(stack) == 1 and stack[0] == 0:\n",
        "            return configurations, gold_transitions\n",
        "\n",
        "        # also, if there are fewer than 2 words on the stack\n",
        "        # and more than 0 left on the buffer, \n",
        "        # we need to perform a shift operation\n",
        "        if len(stack) < 2 and len(wbuffer) > 0:\n",
        "            # shift operations\n",
        "            perform_shift(wbuffer, stack, arcs, configurations, gold_transitions)\n",
        "            continue\n",
        "\n",
        "        # grab s1 and s2\n",
        "        stack1 = stack[-1]\n",
        "        stack2 = stack[-2]\n",
        "\n",
        "        # check against conditions for left arc operation\n",
        "        if stack1 in deps and (stack1, stack2) in deps[stack1]:\n",
        "            # perform left arc\n",
        "            perform_arc(\"LEFT\", deps[stack1][(stack1, stack2)], wbuffer, stack, arcs, configurations, gold_transitions)\n",
        "            # update dep_arcs\n",
        "            dep_arcs[stack2] = 1\n",
        "\n",
        "        # BEGIN SOLUTION  \n",
        "        # check against conditions for right arc operation\n",
        "        elif stack2 in deps and (stack2, stack1) in deps[stack2]:\n",
        "            # check if all the dependents of s1 have been assigned\n",
        "            if stack1 not in deps or all([dep[1] in dep_arcs for dep in deps[stack1].keys()]):\n",
        "                # perform right arc\n",
        "                perform_arc(\"RIGHT\", deps[stack2][(stack2, stack1)], wbuffer, stack, arcs, configurations, gold_transitions)\n",
        "                # update dep_arcs\n",
        "                dep_arcs[stack1] = 1\n",
        "            else:\n",
        "                # perform shift\n",
        "                perform_shift(wbuffer, stack, arcs, configurations, gold_transitions)\n",
        "        # END SOLUTION\n",
        "\n",
        "        # perform shift\n",
        "        else:\n",
        "            perform_shift(wbuffer, stack, arcs, configurations, gold_transitions)\n",
        "    \n",
        "    return configurations, gold_transitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shift attempted!\n",
            "right arc attempted!\n",
            "shift attempted!\n",
            "right arc attempted!\n",
            "right arc attempted!\n",
            "You cleared the sanity check for tree_to_actions()!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "objective: sanity check the tree_to_actions() function\n",
        "\"\"\"\n",
        "def sanity_check_tree_to_actions():\n",
        "\n",
        "    # Setup for invoking tree_to_actions \n",
        "    wbuffer = [9, 8, 7, 6, 5, 4, 3, 2, 1] \n",
        "    stack = [0]\n",
        "    arcs = []\n",
        "    deps = {5: {(5, 9): 'punct', (5, 8): 'obl', (5, 4): 'advmod', (5, 3): 'aux:pass', (5, 2): 'nsubj:pass'},\n",
        "            8: {(8, 7): 'det', (8, 6): 'case'}, 0: {(0, 5): 'root'}, 2: {(2, 1): 'nmod:poss'}}\n",
        "\n",
        "    tree_to_actions(wbuffer, stack, arcs, deps)\n",
        "\n",
        "    # After tree_to_actions\n",
        "    assert wbuffer == [], \"The result for wbuffer is not correct\"\n",
        "    assert stack == [0], \"The result for stack is not correct\"\n",
        "    assert arcs == [('nmod:poss', 2, 1), ('advmod', 5, 4), ('aux:pass', 5, 3), ('nsubj:pass', 5, 2), \n",
        "                     ('det', 8, 7), ('case', 8, 6), ('obl', 5, 8), ('punct', 5, 9), ('root', 0, 5)], \\\n",
        "                        \"The result for arcs is not correct\"\n",
        "    assert deps == {5: {(5, 9): 'punct', (5, 8): 'obl', (5, 4): 'advmod', (5, 3): 'aux:pass', (5, 2): 'nsubj:pass'}, \n",
        "                     8: {(8, 7): 'det', (8, 6): 'case'}, 0: {(0, 5): 'root'}, 2: {(2, 1): 'nmod:poss'}}, \\\n",
        "                    \"The result for deps is not correct\"\n",
        "    print(\"You cleared the sanity check for tree_to_actions()!\")   \n",
        "     \n",
        "sanity_check_tree_to_actions()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0ac904a1c22f75428fe267255c2a419166f9af52fbfab2007fe5f104e18bdf1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
