{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVsWEg6DtYop"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp23/blob/main/HW1/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISij2Cm4tYos"
      },
      "source": [
        "# Homework 1: Featurized Models for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phz-G8pXtYot"
      },
      "source": [
        "For this assignment, we provide an implementation of a simple binary classifier that will predict the sentiment of a movie review based on a group of original features -- provided by you! \n",
        "\n",
        "Before diving into any code, please read through the associated [PDF](https://github.com/dbamman/nlp23/blob/main/HW1/HW1.pdf) for an overview of the assignment and specific instructions on how to submit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "outputs": [],
      "source": [
        "import sys, argparse\n",
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import operator\n",
        "import nltk\n",
        "import csv\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas import option_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e4KuVSCSqlUX",
        "outputId": "3d6f7640-df79-4466-fa9f-3bd99e5d0a4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gecT2RsatYov"
      },
      "source": [
        "### Intro: Gather Data + Create Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcSOvqnttYov"
      },
      "source": [
        "#### Gather Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hk07KCgwoZy"
      },
      "source": [
        "Let's download the data we'll use for training and development, and also the data we'll make predictions with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hn0XtfFeqP2P",
        "outputId": "002360c5-37e4-4795-9340-2920e25a5b22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-31 03:56:14--  https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1322055 (1.3M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]   1.26M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-01-31 03:56:15 (22.7 MB/s) - ‘train.txt’ saved [1322055/1322055]\n",
            "\n",
            "--2023-01-31 03:56:15--  https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1309909 (1.2M) [text/plain]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>]   1.25M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-01-31 03:56:15 (20.4 MB/s) - ‘dev.txt’ saved [1309909/1309909]\n",
            "\n",
            "--2023-01-31 03:56:15--  https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6573426 (6.3M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   6.27M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-01-31 03:56:15 (67.8 MB/s) - ‘test.txt’ saved [6573426/6573426]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get data\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/dev.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp23/main/HW1/test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jq2yq0xpRCUb"
      },
      "outputs": [],
      "source": [
        "trainingFile = \"train.txt\"\n",
        "evaluationFile = \"dev.txt\"\n",
        "testFile = \"test.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jVO-k8xtYox"
      },
      "source": [
        "#### Define Classifier class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wam6cY5qtYox"
      },
      "source": [
        "Next, we've created a Binary Classifier. This class will let us learn the traits associated with positive and negatively classed movie reviews in order to make predictions on our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the classification class which\n",
        "## loads the data and sets up the model.\n",
        "######################################################################\n",
        "\n",
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, L2_regularization_strength=1.0, min_feature_count=1):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.log_reg = None\n",
        "        self.L2_regularization_strength=L2_regularization_strength\n",
        "        self.min_feature_count=min_feature_count\n",
        "\n",
        "        self.trainX, self.trainY, self.trainOrig = self.process(trainingFile, training=True)\n",
        "        self.devX, self.devY, self.devOrig = self.process(evaluationFile, training=False)\n",
        "        self.testX, _, self.testOrig = self.process(testFile, training=False)\n",
        "\n",
        "    # Read data from file\n",
        "    def load_data(self, filename):\n",
        "        data = []\n",
        "        with open(filename, encoding=\"utf8\") as file:\n",
        "            for line in file:\n",
        "                cols = line.split(\"\\t\")\n",
        "                idd = cols[0]\n",
        "                label = cols[1]\n",
        "                text = cols[2]\n",
        "\n",
        "                data.append((idd, label, text))\n",
        "                \n",
        "        return data\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for idd, label, text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append((label, feats))\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, dataFile, training = False):\n",
        "        original_data = self.load_data(dataFile)\n",
        "        data = self.featurize(original_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for label, feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (label, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = label\n",
        "\n",
        "        return X, Y, original_data\n",
        "\n",
        "    def load_test(self, dataFile):\n",
        "        data = self.load_data(dataFile)\n",
        "        data = self.featurize(data)\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (data_id, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = data_id\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    def evaluate(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        self.log_reg = linear_model.LogisticRegression(C = self.L2_regularization_strength, max_iter=1000)\n",
        "        self.log_reg.fit(self.trainX, self.trainY)\n",
        "        training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "        development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "        print(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (self.feature_method.__name__, F, training_accuracy, development_accuracy))\n",
        "\n",
        "\n",
        "    # Predict labels for new data\n",
        "    def predict(self):\n",
        "        predX = self.log_reg.predict(self.testX)\n",
        "\n",
        "        with open(\"%s_%s\" % (self.feature_method.__name__, \"predictions.csv\"), \"w\", encoding=\"utf8\") as out:\n",
        "            writer=csv.writer(out)\n",
        "            writer.writerow([\"Id\", \"Expected\"])\n",
        "            for idx, data_id in enumerate(self.testX):\n",
        "                writer.writerow([self.testOrig[idx][0], predX[idx]])\n",
        "        out.close()\n",
        "\n",
        "\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LDLrVvKtYoy"
      },
      "source": [
        "#### Simple Classifier example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDmfkG782kgo"
      },
      "source": [
        "Let's create an initial classifier based on a really simple feature using a dictionary: \n",
        "\n",
        "* if the abstract contains the words \"love\" or \"like\", the `contains_positive_word` feature will fire, and \n",
        "* if it contains either \"hate\" or \"dislike\", the `contains_negative_word` will fire.  \n",
        "\n",
        "Note how we use `nltk.word_tokenize` to tokenize the text into its discrete words (the documentation for which can be found [here](https://www.nltk.org/api/nltk.tokenize.html))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xCq1bL3e2jUj"
      },
      "outputs": [],
      "source": [
        "def simple_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        if word == \"love\" or word == \"like\":\n",
        "            feats[\"contains_positive_word\"] = 1\n",
        "        if word == \"hate\" or word == \"dislike\":\n",
        "            feats[\"contains_negative_word\"] = 1\n",
        "            \n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3PQdN9r3Ujz"
      },
      "source": [
        "Now let's see how that feature performs on the development data. \n",
        "\n",
        "Note the `L2_regularization_strength` parameter specifies the strength of the L2 regularizer (values closer to 0 = stronger regularization), and `min_feature_count` specifies how many data points need to contain a feature for it to be passed into the model as a feature. Both are ways to prevent the model from overfitting and achieve higher performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jnqjxd6fKPiP",
        "outputId": "ce96afab-bbe2-4d0f-fd48-c09e32e03801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: simple_featurize, Features: 2, Train accuracy: 0.509, Dev accuracy: 0.500\n"
          ]
        }
      ],
      "source": [
        "simple_classifier = Classifier(simple_featurize, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "simple_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO4XQzU3PdeU"
      },
      "source": [
        "So we've created a classifier. But is its accuracy score any good?  Let's calculate the accuracy of a \"majority classifier\" to provide some context. This determines the most-represented (majority) class in the training data, and then predicts every test point to be this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8t--LfOjPj7T",
        "outputId": "fbd31a95-ed79-4f99-bce4-c2551d1bd516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Majority class: pos\tDev accuracy: 0.500\n"
          ]
        }
      ],
      "source": [
        "def majority_class(trainY, devY):\n",
        "    labelCounts=Counter()\n",
        "    for label in trainY:\n",
        "        labelCounts[label]+=1\n",
        "    majority_class=labelCounts.most_common(1)[0][0]\n",
        "    \n",
        "    correct=0.\n",
        "    for label in devY:\n",
        "        if label == majority_class:\n",
        "            correct+=1\n",
        "            \n",
        "    print(\"Majority class: %s\\tDev accuracy: %.3f\" % (majority_class, correct/len(devY)))\n",
        "majority_class(simple_classifier.trainY, simple_classifier.devY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f5ouhZHtYo0"
      },
      "source": [
        "The feature we created in `simple_featurize`, evidently, doesn't have a whole lot of legs. In the next portion of the homework, you'll be designing a few features of your own in the hopes of achieving the highest accuracy possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIEkYOWO5ClC"
      },
      "source": [
        "## Deliverable 1\n",
        "\n",
        "Your job in this homework is to implement a binary bag-of-words model (i.e., one that assigns a feature value of 1 to each word type that is present in the text); and to brainstorm **3 additional** distinct classes of features, justify why they might help improve the performance *over a bag of words* for this task, implement them in code, and then assess their independent performance on the development data.\n",
        "\n",
        "To show your work: describe your new features and report their performance in the table below; implement the features in the specified `feature1`, `feature2`, and `feature3` functions, and execute each respective classifier to show its performance.  \n",
        "\n",
        "|Feature|Why should it work? (50 words each)|Dev set performance|\n",
        "|---|---|---|\n",
        "|Bag of words| Standard bag of words model learns associations between specific words and performance | 0.601\n",
        "|Feature 1| I used stemming and stopwords to decrease the amount of unnecessary and redundant features that may poorly affect discriminatory ability, with a bag of words. After all, too many unnecessary features may lead to overfitting on the training set with logistic regression, which would impact validation performance. | 0.773\n",
        "|Feature 2| Again this approach is similar to bag of words, but I was hoping for more detail in using a lexicon to assign values to the words; essentially moving away from the binary approach implemented in the first two exercises. Also, this lowers the total amount features in this phase, as only those with unique relevance to sentiment are present in the dictionary| 0.749\n",
        "|Feature 3| I tried part of speech tagging using NLTK to see if the distribution of parts of speech differed from positive to negative; perhaps positive reviews focused on parts of the movies and more nouns while some negative reviews had more adjectives; It turned out there was some correlation.| 0.613\n",
        "\n",
        "Note that it is not required for your features to actually perform well, but your justification for why it *should* perform better than a bag of words should be defensible.  The most creative features (defined as features that few other students use and that are reasonably well-performing) will receive extra credit for this assignment. Consider the type of data you are working with: what do you look for when writing/reading a movie review?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UaQGNhdtYo1"
      },
      "source": [
        "### Implement Bag-of-Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vVl1zAREekC3"
      },
      "outputs": [],
      "source": [
        "# nltk.download('stopwords') # use first time\n",
        "def bag_of_words(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    # BEGIN SOLUTION\n",
        "    feats = {}\n",
        "    for word in text:\n",
        "        feats[word.lower()]=1\n",
        "    return feats\n",
        "    # END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_3AJ5qMBeqmL",
        "outputId": "743223b0-42f3-4a51-f5b5-ec93b4fbccb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: bag_of_words, Features: 89, Train accuracy: 0.649, Dev accuracy: 0.601\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "bow_classifier = Classifier(bag_of_words, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "bow_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPVloLH8tYo1"
      },
      "source": [
        "### Implement Original Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ocPMYhIt4BX0"
      },
      "outputs": [],
      "source": [
        "def feature1(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stopword_list=nltk.corpus.stopwords.words('english')\n",
        "    stopword_set = set(stopword_list)\n",
        "    ps = nltk.PorterStemmer()\n",
        "    for word in words:\n",
        "      word = word.lower()\n",
        "      if word not in stopword_set and word.isalpha():\n",
        "        feats[ps.stem(word)]=1\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "-MAwRwbQ7lVw",
        "outputId": "c1537e21-c502-445d-c87a-270ef4e68a40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature1, Features: 12076, Train accuracy: 1.000, Dev accuracy: 0.773\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier1 = Classifier(feature1, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier1.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "LNlQyjEB4Bwt"
      },
      "outputs": [],
      "source": [
        "def feature2(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    afinn_word_dict = {}\n",
        "    with open(\"afinn.txt\", encoding='UTF-8') as fid:\n",
        "        for n, line in enumerate(fid):\n",
        "            try:\n",
        "                word, score = line.strip().split('\\t')\n",
        "            except ValueError:\n",
        "                msg = 'Error in line %d of %s' % (n + 1, \"afinn.txt\")\n",
        "                raise ValueError(msg)\n",
        "            afinn_word_dict[word] = int(score)\n",
        "\n",
        "    scores = []\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stopword_list=nltk.corpus.stopwords.words('english')\n",
        "    stopword_set = set(stopword_list)\n",
        "    for word in words:\n",
        "      word = word.lower()\n",
        "      if word not in stopword_set and word.isalpha():\n",
        "        if word in afinn_word_dict:\n",
        "          feats[f\"{word}_afinn\"] = afinn_word_dict[word]\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "JgpuykF67oWZ",
        "outputId": "fd742801-6838-4d81-a660-bdefd14b7408",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature2, Features: 1804, Train accuracy: 0.994, Dev accuracy: 0.749\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier2 = Classifier(feature2, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier2.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "FmJKucgn4CEg"
      },
      "outputs": [],
      "source": [
        "def feature3(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    scores = []\n",
        "    text = text.lower()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    ps = nltk.PorterStemmer()\n",
        "    tags = nltk.pos_tag(words)\n",
        "    feats = Counter(tag for word, tag in tags)\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "g_f--utb7q4l",
        "outputId": "cf4f301b-253f-42c5-b371-bd3a8ae5d7ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature3, Features: 44, Train accuracy: 0.662, Dev accuracy: 0.613\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "classifier3 = Classifier(feature3, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "classifier3.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg2J1BLgatMP"
      },
      "source": [
        "## Deliverable 2\n",
        "\n",
        "The two cells in \"Combine your features\" will generate a file named `combiner_function_predictions.csv`.\n",
        "\n",
        "Download this file (using e.g. the file manager on the left panel in Colab) and submit this to GradeScope along with your notebook; the 5 students with the highest performance (revealed after the submission deadline) will receive extra credit for this assignment.\n",
        "\n",
        "Please do not change the auto-generated filename!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Me4F5CtYo3"
      },
      "source": [
        "### Combine your features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEpK5LyMgv5c"
      },
      "source": [
        "Next, let's combine any or all the features you have developed into one big model and make predictions on the test data. There is no exact number/threshold we're looking for, accuracy-wise, but the combiner function should *generally* have a higher accuracy than BoW on its own (assuming your features are adding additional information beyond what BoW is adding). \n",
        "\n",
        "You don't need to edit the following cell, unless you want to change which features are handed off to the \"combined\" model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "jxKmEqI5JY71"
      },
      "outputs": [],
      "source": [
        "def combiner_function(text):\n",
        "\n",
        "    # Here the `all_feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    # at the moment, all 4 of: bag of words and your 3 original features are handed off to the combined model\n",
        "    # update the values within [bag_of_words, feature1, feature2, feature3] to change this.\n",
        "    \n",
        "    all_feats={}\n",
        "    for feature in [bag_of_words, feature1, feature2, feature3]:\n",
        "        all_feats.update(feature(text))\n",
        "    return all_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "D-tRUFTIdAqT",
        "outputId": "45c8f0c5-ee84-425f-e76f-b8c245bb7c19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: combiner_function, Features: 13989, Train accuracy: 1.000, Dev accuracy: 0.800\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "\n",
        "big_classifier = Classifier(combiner_function, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "big_classifier.evaluate()\n",
        "\n",
        "#generate .csv file with prediction output on test data\n",
        "big_classifier.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQXmj0Z5tYo3"
      },
      "source": [
        "## Further Exploration: Interrogating classifiers\n",
        "\n",
        "Note: No deliverables are in this section; it's optional. Treat this portion as a useful tool for further understanding the features you worked on and how they affter the classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lgyoJm09pqe"
      },
      "source": [
        "Below you will find several ways in which you can interrogate your model to get ideas on ways to improve its performance.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2pqErvitYo3"
      },
      "source": [
        "1. First, let's look at the confusion matrix of its predictions (where we can compare the true labels with the predicted labels). What kinds of mistakes is it making? (While this is mainly helpful in the context of multiclass classification, we can still see if there's a bias toward predicting a specific class in the binary setting as well). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "7ulxd1TosIMV",
        "outputId": "2c2796ec-2d0f-47f5-e13c-5431f921a207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def print_confusion(classifier):\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    plot_confusion_matrix(classifier.log_reg, classifier.devX, classifier.devY, ax=ax, xticks_rotation=\"vertical\", values_format=\"d\")\n",
        "    plt.show()\n",
        "\n",
        "print_confusion(big_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPhH4flIuEbx"
      },
      "source": [
        "2. Next, let's look at the features that are most important for each of the classes (ranked by how strong their corresponding coefficient is). Do the features you are defining help in the ways you think they should?  Do sets of successful features suggests others, or complementary features that may provide a different view on the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "IAyGuXIi9pqe",
        "outputId": "f0e50ddc-c7a9-4ff6-9e03-36eba50cdd28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos\t0.761\tworst_afinn\n",
            "pos\t0.688\tboring_afinn\n",
            "pos\t0.647\tthought\n",
            "pos\t0.569\tenjoyed_afinn\n",
            "pos\t0.534\tawful_afinn\n",
            "pos\t0.528\tstupid_afinn\n",
            "pos\t0.526\tpoorly_afinn\n",
            "pos\t0.524\thorrible_afinn\n",
            "pos\t0.517\tgreat_afinn\n",
            "pos\t0.505\tgem_afinn\n",
            "pos\t0.497\tworse_afinn\n",
            "pos\t0.493\tseri\n",
            "pos\t0.493\trelationship\n",
            "pos\t0.490\tman\n",
            "pos\t0.486\tpoor_afinn\n",
            "pos\t0.480\tmove\n",
            "pos\t0.475\tseen\n",
            "pos\t0.465\tnew\n",
            "pos\t0.459\tsing\n",
            "pos\t0.448\thuman\n",
            "pos\t0.448\talso\n",
            "pos\t0.444\tfails_afinn\n",
            "pos\t0.439\twonderfully_afinn\n",
            "pos\t0.438\tgo\n",
            "pos\t0.433\tsimpl\n",
            "\n",
            "neg\t-0.658\tact\n",
            "neg\t-0.566\t?\n",
            "neg\t-0.559\tnoth\n",
            "neg\t-0.546\tmayb\n",
            "neg\t-0.531\tidea\n",
            "neg\t-0.530\tEX\n",
            "neg\t-0.528\t4\n",
            "neg\t-0.526\twould\n",
            "neg\t-0.484\twast\n",
            "neg\t-0.465\tbore\n",
            "neg\t-0.460\tmoney\n",
            "neg\t-0.442\tviolence_afinn\n",
            "neg\t-0.440\ttv\n",
            "neg\t-0.437\tdone\n",
            "neg\t-0.429\task\n",
            "neg\t-0.423\tslow\n",
            "neg\t-0.419\thalf\n",
            "neg\t-0.394\tkiller\n",
            "neg\t-0.388\tfoul_afinn\n",
            "neg\t-0.383\twar_afinn\n",
            "neg\t-0.382\tsilli\n",
            "neg\t-0.381\tbrutal_afinn\n",
            "neg\t-0.379\toffic\n",
            "neg\t-0.373\tactor\n",
            "neg\t-0.373\tsometh\n",
            "\n"
          ]
        }
      ],
      "source": [
        "big_classifier.printWeights(n=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80DUsSXu7h9"
      },
      "source": [
        "3. Next, let's look at the individual data points that are the hardest to classify correctly. Does it suggest any features you might create to disentangle them?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "I4uTzwV99pqe"
      },
      "outputs": [],
      "source": [
        "def analyze(classifier):\n",
        "    \n",
        "    probs=classifier.log_reg.predict_proba(classifier.devX)\n",
        "    predicts=classifier.log_reg.predict(classifier.devX)\n",
        "\n",
        "    classes={}\n",
        "    for idx, lab in enumerate(classifier.log_reg.classes_):\n",
        "        classes[lab]=idx\n",
        "\n",
        "    mistakes={}\n",
        "    for i in range(len(probs)):\n",
        "        if predicts[i] != classifier.devY[i]:\n",
        "            predicted_lab_idx=classes[predicts[i]]\n",
        "            mistakes[i]=probs[i][predicted_lab_idx]\n",
        "\n",
        "    frame=[]\n",
        "    sorted_x = sorted(mistakes.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for k, v in sorted_x:\n",
        "        idd=classifier.devOrig[k][0]\n",
        "        text=classifier.devOrig[k][2]\n",
        "        frame.append([idd, v, classifier.devY[k], predicts[k], text])\n",
        "\n",
        "    df=pd.DataFrame(frame, columns=[\"id\", \"P(predicted class confidence)\", \"Human label\", \"Prediction\", \"Text\"])\n",
        "\n",
        "    with option_context('display.max_colwidth', 400):\n",
        "        display(df.head(n=20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "UXmRhSuzxaJi",
        "outputId": "22453adb-9e0e-48db-ebb7-f673c5b90fc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      id  P(predicted class confidence) Human label Prediction  \\\n",
              "0   1436                       1.000000         pos        neg   \n",
              "1   1822                       0.999983         pos        neg   \n",
              "2   1178                       0.999765         pos        neg   \n",
              "3   1670                       0.999755         neg        pos   \n",
              "4   1913                       0.999642         neg        pos   \n",
              "5   1309                       0.999312         neg        pos   \n",
              "6   1959                       0.999233         pos        neg   \n",
              "7   1938                       0.999196         pos        neg   \n",
              "8   1784                       0.999182         pos        neg   \n",
              "9   1110                       0.998964         neg        pos   \n",
              "10  1004                       0.998580         neg        pos   \n",
              "11  1659                       0.998291         pos        neg   \n",
              "12  1218                       0.998288         neg        pos   \n",
              "13  1261                       0.997813         neg        pos   \n",
              "14  1559                       0.997536         neg        pos   \n",
              "15  1328                       0.996351         pos        neg   \n",
              "16  1471                       0.995415         pos        neg   \n",
              "17  1139                       0.994617         pos        neg   \n",
              "18  1956                       0.993952         pos        neg   \n",
              "19  1605                       0.992404         pos        neg   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                               Text  \n",
              "0   I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...  \n",
              "1   Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a sequel to George A. Romero's Dawn of the Dead aka Zombi. Confusing enough? Basically, none of the films have anything to do with one another, but who cares when they make money. I guess Fulci himself start...  \n",
              "2   While I do not think this was a perfect 10, I do agree it was way above a 6 which is what it's rated here. No, Brokedown Palace was not perfect and yes it's plot has been done many times before. That doesn't mean it shouldn't be done again if it is done well and I think this movie had some strong moments. The acting of Claire Danes, as already mentioned many times, was flawless as was Kate Bec...  \n",
              "3   I've always liked Fred MacMurray, andalthough her career was tragically cut shortI think Carole Lombard is fun to watch. Pair these two major and attractive stars together, add top supporting players like Jean Dixon, Anthony Quinn, Dorothy Lamour and Charles Butterworth, give them a romantic script, team them with noted director Mitchell Leisen and you get\n",
              "a mediocre movie experience.Skid Jo...  \n",
              "4   I have only seen Gretchen Mol in two other films (Girl 6, Donnie Brasco), and don't really remember her, but she did a great job as a naive girl who posed for pictures because it made people happy.She really didn't think what she was doing was wrong, even when she left the business and found her religion again.The photos she made were certainly tame by today's standards, and it is funny seeing...  \n",
              "5   Scooby Doo is undoubtedly one of the most simple, successful and beloved cartoon characters in the world. So, what happens when you've been everywhere and done everything with the formula? You switch it up right? Wrong. You stop production and let it rest for a decade or so and then run it again, keeping the core of its success intact. That is to say, stick with the formula for the most part b...  \n",
              "6   This is one of the best episode from the second season of MOH, I think Mick Garris has a problem with women... He kill'em all, they are often the victims (Screwfly solution, Pro-life, Valerie on the stairs, I don't remember the Argento's episode in season 1, etc., obviously Imprint). I think he enjoys to watch women been burn, torture, mutilated and I don't know. Never least \"Right to die\" is ...  \n",
              "7   All right, here's the deal: if you're easily offended then you might want to stay far, far away from this one. There are some painfully funny moments in the movie, but I probably blushed about as much as I laughed. Actually, I probably blushed MORE than I laughed. And if I wasn't literally blushing on the outside, then I was blushing on the inside. If there is absolutely nothing in this movie ...  \n",
              "8   The only complaint I have about this adaptation is that it is sexed-up. Things that were only hinted at in the novel are shown on-screen for some weird reason. Did they think the audience would be too stupid to understand if they were not shown everything out-right? Other than that, this is very good-quality. All the actors do marvelous jobs bringing their characters to life. For the shallow w...  \n",
              "9   Before Stan Laurel became the smaller half of the all-time greatest comedy team, he laboured under contract to Broncho Billy Anderson in a series of cheapies, many of which were parodies of major Hollywood features. Following a dispute with Anderson, Laurel continued the informal series of parodies at Joe Rock's smaller (and more indigent) production company.Most of Laurel's parody films were ...  \n",
              "10  Certainly NOMAD has some of the best horse riding scenes, swordplay, and scrumptious landscape cinematography you'll likely see, but this isn't what makes a film good. It helps but the story has to shine through on top of these things. And that's where Nomad wanders.The story is stilted, giving it a sense that it was thrown together simply to make a \"cool\" movie that \"looks\" great. Not to ment...  \n",
              "11  Child 'Sexploitation' is one of the most serious issues facing our world today and I feared that any film on the topic would jump straight to scenes of an explicitly sexual nature in order to shock and disturb the audience. After having seen both 'Trade' and 'Holly', one film moved me to want to actually see a change in international laws. The other felt like a poor attempt at making me cry fo...  \n",
              "12  This early B entry into the patriotic category slapped a gorgeous young Gene Tierney on the ads and posters, but you have to wait a good time before you glimpse her, riding in a Hollywoodized camel train. Previously, we've set up George Sanders and Bruce Cabot in the desert as guys who barely get along, but must rally in the face of attack. I've seen Sanders as so many enjoyable cads that it w...  \n",
              "13  NATURAL BORN KILLERS (1994)Cinema Cut: R Director's Cut: NC-17It's an unusual Oliver Stone picture, but when I read he was on drugs during the filming, I needed no further explanation. 'Natural Born Killers' is a risky, mad, all out film-making that we do not get very often; strange, psychotic, artistic pictures.'Natural Born Killers' is basically the story of how two mass killers were popular...  \n",
              "14  A young scientist Harry Harrison is continuing his late father's scientific research into limb regeneration with flying colours, but his interferingly dominate mother and her doctor lover want to sell off the serum. When he finds out, there is an accident involving Harry losing an arm. So, he tries out the serum and what eventuates is a genetically deranged arm that has a mind of its own.Oh we...  \n",
              "15  I have screened this movie several times here at college, and every time I show it, the number of people watching with me grows exponentially... in addition to the virgins, anyone I've already shown it to NEEDS to see it again! It takes a little while to get into it, but by the end the whole room is screaming, shouting, yelling, rewinding scenes repeatedly, repeating dialogue, and just totally...  \n",
              "16  But the opposite, sorry bud, i completely understand how you can be dragged into a film because you relate to the subject ( and you have). This film is terrible, the main character would give any charlie brown subtitler a run for his money he just constantly mumbles which is always a laugh, most scenes just feel awkward with characters more often than not gazing across to another with a look o...  \n",
              "17  I saw this movie as a kid on Creature Feature when I lived in New York. It was a pretty creepy movie, though not as good as Horror Hotel. I just bought this movie on DVD, and it is different from what I remember because in the DVD that I bought there are several scenes where the actors speak in French and/or Italian and no subtitles are provided. Then the other actors respond in English to wha...  \n",
              "18  NOTHING (3+ outta 5 stars) Another weird premise from the director of the movie \"Cube\". This time around there are two main characters who find themselves and their home transported to a mysterious white void. There is literally NOTHING outside of their small two-story house. Intriguing to be sure, but I thought the comedic tone established for this movie from the get-go was extremely ill-conc...  \n",
              "19  This interesting Giallo boosts a typical but still thrilling plot and a really sadistic killer that obviously likes to hunt his victims down before murdering them in gory ways.Directed by Emilio P. Miraglia who, one year earlier, also made the very interesting \"La Notte che Evelyn Usci della Tomba\" (see also my comment on that one), the film starts off a little slow, but all in all, no time is...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e788407-cc46-460b-bd36-151ae4844da7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>P(predicted class confidence)</th>\n",
              "      <th>Human label</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1436</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The story behind the movie is decent but the movies scenes fail to portray them. I have never seen such a stupid movie in my life which is why it I think its worth watching. I give this movie 10 out of 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1822</td>\n",
              "      <td>0.999983</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a sequel to George A. Romero's Dawn of the Dead aka Zombi. Confusing enough? Basically, none of the films have anything to do with one another, but who cares when they make money. I guess Fulci himself start...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1178</td>\n",
              "      <td>0.999765</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>While I do not think this was a perfect 10, I do agree it was way above a 6 which is what it's rated here. No, Brokedown Palace was not perfect and yes it's plot has been done many times before. That doesn't mean it shouldn't be done again if it is done well and I think this movie had some strong moments. The acting of Claire Danes, as already mentioned many times, was flawless as was Kate Bec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1670</td>\n",
              "      <td>0.999755</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I've always liked Fred MacMurray, andalthough her career was tragically cut shortI think Carole Lombard is fun to watch. Pair these two major and attractive stars together, add top supporting players like Jean Dixon, Anthony Quinn, Dorothy Lamour and Charles Butterworth, give them a romantic script, team them with noted director Mitchell Leisen and you geta mediocre movie experience.Skid Jo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1913</td>\n",
              "      <td>0.999642</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I have only seen Gretchen Mol in two other films (Girl 6, Donnie Brasco), and don't really remember her, but she did a great job as a naive girl who posed for pictures because it made people happy.She really didn't think what she was doing was wrong, even when she left the business and found her religion again.The photos she made were certainly tame by today's standards, and it is funny seeing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1309</td>\n",
              "      <td>0.999312</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Scooby Doo is undoubtedly one of the most simple, successful and beloved cartoon characters in the world. So, what happens when you've been everywhere and done everything with the formula? You switch it up right? Wrong. You stop production and let it rest for a decade or so and then run it again, keeping the core of its success intact. That is to say, stick with the formula for the most part b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1959</td>\n",
              "      <td>0.999233</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>This is one of the best episode from the second season of MOH, I think Mick Garris has a problem with women... He kill'em all, they are often the victims (Screwfly solution, Pro-life, Valerie on the stairs, I don't remember the Argento's episode in season 1, etc., obviously Imprint). I think he enjoys to watch women been burn, torture, mutilated and I don't know. Never least \"Right to die\" is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1938</td>\n",
              "      <td>0.999196</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>All right, here's the deal: if you're easily offended then you might want to stay far, far away from this one. There are some painfully funny moments in the movie, but I probably blushed about as much as I laughed. Actually, I probably blushed MORE than I laughed. And if I wasn't literally blushing on the outside, then I was blushing on the inside. If there is absolutely nothing in this movie ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1784</td>\n",
              "      <td>0.999182</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>The only complaint I have about this adaptation is that it is sexed-up. Things that were only hinted at in the novel are shown on-screen for some weird reason. Did they think the audience would be too stupid to understand if they were not shown everything out-right? Other than that, this is very good-quality. All the actors do marvelous jobs bringing their characters to life. For the shallow w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1110</td>\n",
              "      <td>0.998964</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Before Stan Laurel became the smaller half of the all-time greatest comedy team, he laboured under contract to Broncho Billy Anderson in a series of cheapies, many of which were parodies of major Hollywood features. Following a dispute with Anderson, Laurel continued the informal series of parodies at Joe Rock's smaller (and more indigent) production company.Most of Laurel's parody films were ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1004</td>\n",
              "      <td>0.998580</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Certainly NOMAD has some of the best horse riding scenes, swordplay, and scrumptious landscape cinematography you'll likely see, but this isn't what makes a film good. It helps but the story has to shine through on top of these things. And that's where Nomad wanders.The story is stilted, giving it a sense that it was thrown together simply to make a \"cool\" movie that \"looks\" great. Not to ment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1659</td>\n",
              "      <td>0.998291</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Child 'Sexploitation' is one of the most serious issues facing our world today and I feared that any film on the topic would jump straight to scenes of an explicitly sexual nature in order to shock and disturb the audience. After having seen both 'Trade' and 'Holly', one film moved me to want to actually see a change in international laws. The other felt like a poor attempt at making me cry fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1218</td>\n",
              "      <td>0.998288</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>This early B entry into the patriotic category slapped a gorgeous young Gene Tierney on the ads and posters, but you have to wait a good time before you glimpse her, riding in a Hollywoodized camel train. Previously, we've set up George Sanders and Bruce Cabot in the desert as guys who barely get along, but must rally in the face of attack. I've seen Sanders as so many enjoyable cads that it w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1261</td>\n",
              "      <td>0.997813</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>NATURAL BORN KILLERS (1994)Cinema Cut: R Director's Cut: NC-17It's an unusual Oliver Stone picture, but when I read he was on drugs during the filming, I needed no further explanation. 'Natural Born Killers' is a risky, mad, all out film-making that we do not get very often; strange, psychotic, artistic pictures.'Natural Born Killers' is basically the story of how two mass killers were popular...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1559</td>\n",
              "      <td>0.997536</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>A young scientist Harry Harrison is continuing his late father's scientific research into limb regeneration with flying colours, but his interferingly dominate mother and her doctor lover want to sell off the serum. When he finds out, there is an accident involving Harry losing an arm. So, he tries out the serum and what eventuates is a genetically deranged arm that has a mind of its own.Oh we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1328</td>\n",
              "      <td>0.996351</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I have screened this movie several times here at college, and every time I show it, the number of people watching with me grows exponentially... in addition to the virgins, anyone I've already shown it to NEEDS to see it again! It takes a little while to get into it, but by the end the whole room is screaming, shouting, yelling, rewinding scenes repeatedly, repeating dialogue, and just totally...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1471</td>\n",
              "      <td>0.995415</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>But the opposite, sorry bud, i completely understand how you can be dragged into a film because you relate to the subject ( and you have). This film is terrible, the main character would give any charlie brown subtitler a run for his money he just constantly mumbles which is always a laugh, most scenes just feel awkward with characters more often than not gazing across to another with a look o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1139</td>\n",
              "      <td>0.994617</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I saw this movie as a kid on Creature Feature when I lived in New York. It was a pretty creepy movie, though not as good as Horror Hotel. I just bought this movie on DVD, and it is different from what I remember because in the DVD that I bought there are several scenes where the actors speak in French and/or Italian and no subtitles are provided. Then the other actors respond in English to wha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1956</td>\n",
              "      <td>0.993952</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>NOTHING (3+ outta 5 stars) Another weird premise from the director of the movie \"Cube\". This time around there are two main characters who find themselves and their home transported to a mysterious white void. There is literally NOTHING outside of their small two-story house. Intriguing to be sure, but I thought the comedic tone established for this movie from the get-go was extremely ill-conc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1605</td>\n",
              "      <td>0.992404</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>This interesting Giallo boosts a typical but still thrilling plot and a really sadistic killer that obviously likes to hunt his victims down before murdering them in gory ways.Directed by Emilio P. Miraglia who, one year earlier, also made the very interesting \"La Notte che Evelyn Usci della Tomba\" (see also my comment on that one), the film starts off a little slow, but all in all, no time is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e788407-cc46-460b-bd36-151ae4844da7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7e788407-cc46-460b-bd36-151ae4844da7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7e788407-cc46-460b-bd36-151ae4844da7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "nalyze(big_classifier)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "HW1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}